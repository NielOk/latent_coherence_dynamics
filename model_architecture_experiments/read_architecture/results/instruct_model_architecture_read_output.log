A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:
- configuration_llada.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:
- modeling_llada.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-05-04 22:33:16.008987: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-04 22:33:16.247874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746397996.378919    4103 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746397996.418482    4103 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-04 22:33:16.655454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]Fetching 6 files:  17%|█▋        | 1/6 [00:20<01:44, 20.92s/it]Fetching 6 files:  33%|███▎      | 2/6 [00:30<00:57, 14.25s/it]Fetching 6 files: 100%|██████████| 6/6 [00:30<00:00,  5.08s/it]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 89.63it/s]
 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDAModelLM'>
model <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDAModel'>
model.transformer <class 'torch.nn.modules.container.ModuleDict'>
model.transformer.wte <class 'torch.nn.modules.sparse.Embedding'>
model.transformer.emb_drop <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.ln_f <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks <class 'torch.nn.modules.container.ModuleList'>
model.transformer.blocks.0 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.0.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.0.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.0.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.0.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.0.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.0.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.0.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.1.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.1.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.1.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.1.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.1.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.1.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.1.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.2.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.2.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.2.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.2.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.2.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.2.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.2.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.3.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.3.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.3.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.3.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.3.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.3.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.3.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.4.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.4.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.4.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.4.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.4.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.4.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.4.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.5.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.5.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.5.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.5.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.5.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.5.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.5.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.6.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.6.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.6.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.6.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.6.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.6.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.6.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.7.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.7.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.7.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.7.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.7.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.7.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.7.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.8.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.8.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.8.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.8.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.8.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.8.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.8.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.9.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.9.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.9.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.9.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.9.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.9.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.9.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.10.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.10.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.10.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.10.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.10.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.10.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.10.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.11.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.11.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.11.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.11.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.11.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.11.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.11.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.12.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.12.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.12.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.12.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.12.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.12.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.12.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.13.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.13.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.13.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.13.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.13.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.13.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.13.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.14.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.14.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.14.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.14.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.14.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.14.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.14.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.15.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.15.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.15.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.15.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.15.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.15.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.15.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.16.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.16.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.16.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.16.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.16.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.16.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.16.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.17.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.17.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.17.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.17.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.17.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.17.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.17.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.18.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.18.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.18.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.18.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.18.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.18.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.18.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.19.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.19.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.19.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.19.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.19.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.19.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.19.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.20.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.20.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.20.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.20.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.20.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.20.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.20.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.21.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.21.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.21.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.21.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.21.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.21.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.21.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.22.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.22.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.22.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.22.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.22.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.22.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.22.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.23.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.23.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.23.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.23.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.23.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.23.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.23.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.24.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.24.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.24.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.24.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.24.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.24.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.24.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.25.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.25.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.25.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.25.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.25.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.25.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.25.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.26.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.26.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.26.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.26.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.26.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.26.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.26.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.27.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.27.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.27.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.27.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.27.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.27.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.27.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.28.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.28.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.28.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.28.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.28.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.28.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.28.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.29.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.29.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.29.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.29.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.29.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.29.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.29.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.30.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.30.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.30.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.30.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.30.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.30.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.30.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31 <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.LLaDALlamaBlock'>
model.transformer.blocks.31.dropout <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.Dropout'>
model.transformer.blocks.31.act <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.SiLU'>
model.transformer.blocks.31.attn_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.ff_out <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.rotary_emb <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RotaryEmbedding'>
model.transformer.blocks.31.attn_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.31.ff_norm <class 'transformers_modules.GSAI-ML.LLaDA-8B-Instruct.9275bf8f5a5687507189baf4657e91c51b2be338.modeling_llada.RMSLayerNorm'>
model.transformer.blocks.31.q_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.k_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.v_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.ff_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.blocks.31.up_proj <class 'torch.nn.modules.linear.Linear'>
model.transformer.ff_out <class 'torch.nn.modules.linear.Linear'>
